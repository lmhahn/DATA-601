{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nt2eGmdOKCmL"
   },
   "source": [
    "<div class = \"alert alert-block alert-info\">\n",
    "    \n",
    "## DATA 601 - FALL 2025\n",
    "### PROJECT 1\n",
    "### NAME: LAURA HAHN\n",
    "### DATE: 11/17/2025\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Instructions\n",
    "The input_data folder has various csv files related to MD crash data, weather data, total drivers by state and state names.\n",
    "\n",
    "- MD_Crash_report files are vehicle crash report data obtained from for https://opendata.maryland.gov. These include four data tables on accident crash reports, driver (person) data, emergency medical services (EMS) data, and vehicle data. All tables have a common ID feature called \"Report Number\". The data includes the following years and quarters:\n",
    "    - [CY2017Q1](https://opendata.maryland.gov/Public-Safety/Maryland-Statewide-Vehicle-Crashes-CY2017-Quarter-/9886-wded) \n",
    "    - [CY2017Q2](https://opendata.maryland.gov/Public-Safety/Maryland-Statewide-Vehicle-Crashes-CY2017-Quarter-/wnvu-hisq)\n",
    "    - [CY2017Q3](https://opendata.maryland.gov/Public-Safety/Maryland-Statewide-Vehicle-Crashes-CY2017-Quarter-/m6zc-qj6d)\n",
    "    - [CY2017Q4](https://opendata.maryland.gov/Public-Safety/Maryland-Statewide-Vehicle-Crashes-CY2017-Quarter-/9tum-d4as)\n",
    "- States_drivers.csv includes data on total drivers by state.\n",
    "- States.json includes the state name and abbreviations in case it is needed.\n",
    "\n",
    "Other documentation:\n",
    "- Sample __State of Maryland Vehicle Accident Report__ is included in the file \"maryland_accident_report.pdf\" (source: https://www.nhtsa.gov/sites/nhtsa.gov/files/documents/maryland_0.pdf). This report has several codes that can be used to explain codes in the data.\n",
    "- The __Maryland Cross Reference Document__ is included in the file \"maryland_cross_reference_document.pdf\" (source: https://www.nhtsa.gov/sites/nhtsa.gov/files/documents/maryland_cross_reference.pdf). This document may also provide some more details in the codes in the data.\n",
    "- The __maryland county codes__ is included in the file \"MD_County_Codes.pdf\" (source: https://mde.maryland.gov/programs/Land/Documents/LeadFactSheets/LeadfsCountyCodes.pdf)\n",
    "\n",
    "<br>\n",
    "Document any assumptions that you do as part of a comment or a markdown cell were appropriate. Note that you may change the format of the project as long as the questions are answered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Cm77Wu6KCmN"
   },
   "source": [
    "### Question 1 (15 points)\n",
    "\n",
    "Review the questions below to familiarize yourself with the task. After reviewing the questions review the data files of the MD_Crash_Reports of the project and determine if it is worth merging/concatenation/joining and document your reasoning on merging or not merging the crash datafiles that will be used. Question 1 and 2 may be combined for purposes of organizing the Project 1 Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I would not merge the data sets during the initial data exploration. \n",
    "#It will be difficult to merge based on report number because the vehicle and person file reference mulitple entities involved in a single crash\n",
    "#I also worry that merging the raw datasets would create a file size that uses too much memory and is difficult to manipulate\n",
    "#The crash file provides the conditions and location of the crash\n",
    "#The vehicle file provides data regarding the vehicles involved in the crash\n",
    "#The person file provides details regarding possible human factors leading to the crash\n",
    "#This categorization of the data makes it easier to initially explore and become familiar with the variables I may need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (15 points)\n",
    "Use the .info(), .describe(), .hist() and sns.pairplot() functions to explore the data. Comment on any observations and identify any features that may be worth plotting and evaluating in more detail. (Note that the last question requires to develop a visualization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from math import radians, sin, cos, acos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading vehicle file data\n",
    "df_vehiclefile = pd.read_csv('./input_data/MD_Crash_Vehicle_Data_2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring vehicle data\n",
    "df_vehiclefile.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_vehiclefile.describe(include = 'object').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehiclefile.describe(include = 'float64').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehiclefile.describe(include = 'int64').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_vehiclefile['HARM_EVENT_CODE'].hist(range = (0, 4), grid = True, bins = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehiclefile['VEH_YEAR'].hist(range = (1980, 2020), grid = True, bins = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-block alert-info\">\n",
    "\n",
    "### Observations on the Vehicle File\n",
    "1. Many columns can be excluded for the purposes of answering the questions outlined below. \n",
    "2. Based on the reference document for the data set, several data columns are not consistent with the variable names that should be reported in the file, including: 'DR-CDL' (now 'COMMERCIAL FLAG'), 'GOING' (now 'GOING DIRECTION CODE'), 'DR_STATE' (not present), 'CV_HZM_NUM' (now HZM_NUM), '1ST_IMPACT' (not present), 'MAIN_IMPACT (not present), 'NUM_OCC' (not present), 'PLT_STATE' (not present), 'DAMAGE' (now \"DAMAGE_CODE', and referring to damage severity)\n",
    "3. Datatype is 'float' for many columns of integer codes\n",
    "4. Redundant variables:'AREA_DAMAGED'columns\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading crash file data; converted from Excel file type to CSV\n",
    "df_crashfile = pd.read_csv('./input_data/MD_Crash_Data_2017_CSV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring crash file data\n",
    "df_crashfile.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crashfile.describe(include = 'object').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crashfile.describe(include = 'float64').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crashfile.describe(include = 'int64').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_crashfile['WEATHER_CODE'].hist(range = (0, 10), grid = True, bins = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-block alert-info\">\n",
    "\n",
    "### Observations on the Crash File:\n",
    "1. The weather codes do not match the reference documentation range: 01-04\n",
    "2. Several variables with float data type that should be integers\n",
    "3. Datetime variables that will need converted\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading person file data\n",
    "df_personfile = pd.read_csv('./input_data/MD_Crash_Person_Data_2017_CSV.csv')\n",
    "df_personID = pd.read_csv('./input_data/MD_Crash_Person_ID_Data_2017_CSV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging person data\n",
    "df_personfile_merge = df_personfile.merge(df_personID, how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring person file data\n",
    "df_personfile_merge.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_personfile_merge.describe(include = 'object').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_personfile_merge.describe(include = 'float64').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_personfile_merge.describe(include = 'int64').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_personfile_merge.hist('INJ_SEVER_CODE', range = (0,6), bins = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-block alert-info\">\n",
    "\n",
    "### Observations on the Person File\n",
    "1. Based on the histogram, most crashes resulted in no injury.\n",
    "2. There are 24 instances of: PERSON_ID = 3aa35b8c-a096-45e9-98ed-aea365536e71 in the data set; likely duplicates\n",
    "3. There are 1115 intances of the VEHICLE_ID: 17092927-3368-47f0-ae9d-e38aebf9f0d1\tin the data set; likely duplicates\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (15 points)\n",
    "Clean the data as you seem necessary. For example, evaluate remove null or duplicate values. Note there may be features where it may be appropriate to have values that are null or duplicated values. Document your reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for duplicates in crash file\n",
    "duplicates_crash = df_crashfile[df_crashfile.duplicated(keep = False)].copy() \n",
    "duplicates_crash.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for duplicates in vehicle file\n",
    "duplicates_vehicle = df_vehiclefile[df_vehiclefile.duplicated(keep = False)].copy() \n",
    "duplicates_vehicle.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for duplicates in the person file\n",
    "duplicates_person = df_personfile_merge[df_personfile_merge.duplicated(keep = False)].copy() \n",
    "duplicates_person.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping duplicate rows in the person file\n",
    "print(df_personfile_merge.shape[0])\n",
    "df_personfile_merge = df_personfile_merge.drop_duplicates(keep = 'first').reset_index(drop=True)\n",
    "print(df_personfile_merge.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#At this point I am going to drop all columns from the files that I don't need to answer the questions below\n",
    "col_drop_vehicle = ['CONTI_DIRECTION_CODE', 'DAMAGE_CODE','MOVEMENT_CODE','CV_BODY_TYPE_CODE','COMMERCIAL_FLAG', 'HZM_NUM',\n",
    "                    'TOWED_AWAY_FLAG', 'GOING_DIRECTION_CODE', 'BODY_TYPE_CODE', 'DRIVERLESS_FLAG','FIRE_FLAG','HIT_AND_RUN_FLAG',\n",
    "                    'HAZMAT_SPILL_FLAG', 'VEHICLE_ID', 'TOWED_VEHICLE_CONFIG_CODE', 'AREA_DAMAGED_CODE_IMP1','AREA_DAMAGED_CODE1', \n",
    "                    'AREA_DAMAGED_CODE2', 'AREA_DAMAGED_CODE3', 'AREA_DAMAGED_CODE_MAIN'] \n",
    "                    \n",
    "col_drop_crash = ['MUNI_CODE','JUNCTION_CODE', 'COLLISION_TYPE_CODE', 'LANE_CODE', 'RD_COND_CODE', 'RD_DIV_CODE', \n",
    "                  'FIX_OBJ_CODE', 'REPORT_TYPE', 'LOC_CODE', 'SIGNAL_FLAG', 'C_M_ZONE_FLAG', 'AGENCY_CODE', 'AREA_CODE',\n",
    "                  'RTE_NO', 'ROUTE_TYPE_CODE', 'RTE_SUFFIX', 'LOG_MILE', 'LOGMILE_DIR_FLAG', 'MAINROAD_NAME', 'DISTANCE',\n",
    "                  'FEET_MILES_FLAG', 'DISTANCE_DIR_FLAG', 'REFERENCE_NO', 'REFERENCE_TYPE_CODE', 'REFERENCE_SUFFIX', 'REFERENCE_ROAD_NAME']\n",
    "                  \n",
    "col_drop_person = ['OCC_SEAT_POS_CODE', 'PED_VISIBLE_CODE', 'PED_LOCATION_CODE', 'PED_OBEY_CODE', 'MOVEMENT_CODE', 'ALCOHOL_TEST_CODE', \n",
    "                   'ALCOHOL_TESTTYPE_CODE', 'DRUG_TEST_CODE', 'DRUG_TESTRESULT_CODE', 'BAC_CODE', 'FAULT_FLAG', 'EQUIP_PROB_CODE', \n",
    "                   'SAF_EQUIP_CODE', 'CONDITION_CODE','EJECT_CODE', 'CLASS', 'CDL_FLAG', 'EMS_UNIT_LABEL','PED_TYPE_CODE', \n",
    "                   'AIRBAG_DEPLOYED', 'VEHICLE_ID'] \n",
    "\n",
    "df_vehiclefile.drop(columns = col_drop_vehicle, inplace = True)\n",
    "df_crashfile.drop(columns = col_drop_crash, inplace = True)\n",
    "df_personfile_merge.drop(columns = col_drop_person, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examining null values in the crash file\n",
    "df_crashfile.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The surface condition null values represent unreported codes in the police report\n",
    "#The reference documentation guide indicates that unknown conditions should be marked as '99'\n",
    "#So I'll impute the value '99' for all null values\n",
    "df_crashfile['SURF_COND_CODE'] = df_crashfile['SURF_COND_CODE'].fillna(99.0)\n",
    "df_crashfile.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examinig null values in vehicle file\n",
    "df_vehiclefile.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is a relatively high percentage of missing data for the vehicle year\n",
    "print(((df_vehiclefile['VEH_YEAR'].isnull().sum())/(df_vehiclefile.shape[0])*100), 'percent of all vehicle year values are null values.')\n",
    "print('The average vehicle year documented for collisions in Maryland in 2017 is:', df_vehiclefile['VEH_YEAR'].mean())\n",
    "#I'm concerned that imputing the average may artifically skew the distribution:\n",
    "df_vehiclefile['VEH_YEAR'].hist(range = (1980, 2020), grid = True, bins = 20)\n",
    "#I'll leave these null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'll impute the value \"Unknown\" for 'VEH MAKE' and 'VEH MODEL'\n",
    "df_vehiclefile['VEH_MAKE'] = df_vehiclefile['VEH_MAKE'].fillna('Unknown')\n",
    "df_vehiclefile['VEH_MODEL'] = df_vehiclefile['VEH_MODEL'].fillna('Unknown')\n",
    "df_vehiclefile.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking null values in person file\n",
    "df_personfile_merge.isnull().sum()\n",
    "#I can't infer anything or impute any values for these null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting data types in crash file -- this will truncate the trailing decimals of undetermined significance in the code values\n",
    "df_crashfile = df_crashfile.astype({'LIGHT_CODE': 'int64'})\n",
    "df_crashfile = df_crashfile.astype({'WEATHER_CODE': 'int64'})\n",
    "df_crashfile = df_crashfile.astype({'SURF_COND_CODE': 'int64'})\n",
    "df_crashfile = df_crashfile.astype({'ACC_DATE': 'datetime64[ns]'})\n",
    "df_crashfile = df_crashfile.astype({'HARM_EVENT_CODE1':'int64'})\n",
    "df_crashfile = df_crashfile.astype({'HARM_EVENT_CODE2': 'int64'})\n",
    "\n",
    "#I'm going to keep the 'ACC_TIME' as an object for now because it does not have an accompanying date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking data tpyes of columns\n",
    "df_crashfile.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7akHII8bKCmO"
   },
   "source": [
    "### Question 4 (15 points)\n",
    "a. For each county, determine the average speed of accidents involving bicyclists. (Note: Take a look at the Sample Maryland Crash Report. Report provides a map of the codes including those involving other bicyclists.) <br>\n",
    "b. Sort the list the sorted by average speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data set does not include enough information to estimate the speed vehicles were traveling before impact\n",
    "#Though posted speed limit ('SPEED_LIMIT') may correlate with an approximate range of vehicle speed in a collision, this is unknown\n",
    "#The \"PARKED_FLAG\" could be used as a proxy for vehicle speed = 0 mph, but no other proxies are in this dataset\n",
    "#\"Black box\" data from the vehicle's computer would be the only way to determine the speed at impact\n",
    "#I can instead provide the bike accident data sorted by average posted speed limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I will first start with filtering by'HARM-EVENT_CODE\" from the crash file\n",
    "#This will provide a unique list of report numbers that I can then use to analyze the vehicle file\n",
    "df_crashfile_filtered = df_crashfile[(df_crashfile['HARM_EVENT_CODE1'] == 4) | (df_crashfile['HARM_EVENT_CODE2'] == 4)].reset_index(drop = True)\n",
    "df_crashfile_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging based on report numbers from filtered crash file\n",
    "bike_accidents_merge = df_crashfile_filtered.merge(df_vehiclefile, how = 'left')\n",
    "bike_accidents_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping the merged list by county and calculating average speed limit for each county\n",
    "bike_accidents_bycounty = bike_accidents_merge.groupby('COUNTY_NO')['SPEED_LIMIT'].mean().reset_index()\n",
    "#Sorting list by average speed limit\n",
    "bike_accidents_bycounty.sort_values('SPEED_LIMIT', ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yh9NLokhKCmO"
   },
   "source": [
    "### Question 5 (15 points)\n",
    "a. Determine the total number of crashes per \"vehicle make\". Sort the results in alphabetical order by vehicle make (e.g. Chevrolet, Ford, Honda, etc.). <br>\n",
    "b. Determine average age of the car involved in these accidents for each \"vehicle make\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'm going to first explore the VEH_MAKE column to decide the best way to clean/sort it\n",
    "vehmake_series = df_vehiclefile['VEH_MAKE']\n",
    "vehmake_series.sort_values(ascending = True).head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observations\n",
    "##Many of the data points are miscategorized due to misspellings or abbreviations of the vehicle make\n",
    "##I'll need to clean this column of data more extensively\n",
    "##There are so many erroneous vehicle make names in the data set that I don't believe I can efficiently correct all of them\n",
    "##For now, I will attempt to fix at least the top vehicle manufacturer names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a new function to fix the vehicle make names\n",
    "\n",
    "def clean_vehicle_make(name):\n",
    "    cleaned_name = str(name).strip().replace('\\t', '').replace('-', '').replace('_', '').replace('20', 'x').replace('4', 'x').replace('0','x')\n",
    "    cleaned_name = cleaned_name.upper()\n",
    "    if 'CHEV' in cleaned_name:\n",
    "        return 'CHEVROLET'\n",
    "    elif 'HOND' in cleaned_name:\n",
    "        return 'HONDA'\n",
    "    elif 'HYUN' in cleaned_name:\n",
    "        return 'HYUNDAI'\n",
    "    elif 'NISS' in cleaned_name:\n",
    "        return 'NISSAN'\n",
    "    elif 'ACU' in cleaned_name:\n",
    "        return 'ACURA'\n",
    "    elif 'TOY' in cleaned_name:\n",
    "        return 'TOYOTA'\n",
    "    elif 'CAD' in cleaned_name:\n",
    "        return 'CADILLAC'\n",
    "    elif 'INF' in cleaned_name:\n",
    "        return 'INFINITI'\n",
    "    elif 'AUD' in cleaned_name:\n",
    "        return 'AUDI'\n",
    "    elif 'BUI' in cleaned_name:\n",
    "        return 'BUICK'\n",
    "    elif 'CHRY' in cleaned_name:\n",
    "        return 'CHRYSLER'\n",
    "    elif 'DOD' in cleaned_name:\n",
    "        return 'DODGE'\n",
    "    elif 'FORD'in cleaned_name:\n",
    "        return 'FORD'\n",
    "    elif 'LEX' in cleaned_name:\n",
    "        return 'LEXUS'\n",
    "    elif 'LINC' in cleaned_name:\n",
    "        return 'LINCOLN'\n",
    "    elif 'MERC' in cleaned_name:\n",
    "        return 'MERCEDES'\n",
    "    elif 'MITSU' in cleaned_name:\n",
    "        return 'MITSUBISHI'\n",
    "    elif 'SUB' in cleaned_name:\n",
    "        return 'SUBARU'\n",
    "    else:\n",
    "        return cleaned_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying my function to vehicle make names and making new column of cleaned names\n",
    "df_vehiclefile['VEH_MAKE_CLEAN'] = df_vehiclefile['VEH_MAKE'].apply(clean_vehicle_make)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer to part (a):\n",
    "sorted_vehmake = df_vehiclefile.groupby('VEH_MAKE_CLEAN').agg({'REPORT_NO': 'count'}).sort_values(by = 'VEH_MAKE_CLEAN', ascending = True)\n",
    "sorted_vehmake.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer to part (b):\n",
    "sorted_vehyear = df_vehiclefile.groupby('VEH_MAKE_CLEAN').agg({'VEH_YEAR': 'mean'}).sort_values(by = ['VEH_MAKE_CLEAN'], ascending = True)\n",
    "sorted_vehyear.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33AowxTGKCmP"
   },
   "source": [
    "### Question 6 (15 points)\n",
    "a. Determine all the accidents that happened in a circle centered at (39.197753, -77.263303) with a radius of 5 km.<br>\n",
    "b. Determine the number of accidents where the speed limit was above 51 miles per hour within the 5 km circle. <br>\n",
    "c. Determine the number of accidents where the speed limit was below 41 miles per hour within the 5 km circle.  <br>\n",
    "<br>\n",
    "Hint: Please see the \"P1_Helper_Distance_From_Coordinates.ipynb\" that can be found in the Project1 folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Defining function to filter crash file to include accidents in 5km radius\n",
    "def haversine_distance(lat2, lon2):\n",
    "    r = 6371\n",
    "    lat1 = 39.197753\n",
    "    lon1 = -77.263303\n",
    "    phi1 = np.radians(lat1)\n",
    "    phi2 = np.radians(lat2)\n",
    "    delta_phi = np.radians(lat2-lat1)\n",
    "    delta_lambda = np.radians(lon2-lon1)\n",
    "    a = np.sin(delta_phi / 2)**2 + np.cos(phi1) * np.cos(phi2) *   np.sin(delta_lambda / 2)**2\n",
    "    res = r * (2 * np.arctan2(np.sqrt(a), np.sqrt(1-a)))\n",
    "    distance = np.round(res, 2)\n",
    "    if distance <= 5.00:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying function to crashfile; using inline lambda function to ensure function applied row by row\n",
    "df_crashfile['IN_CIRCLE'] = df_crashfile.apply(lambda row: haversine_distance(row['LATITUDE'], row['LONGITUDE']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering by resulting bool value of \"True\" in the new IN_CIRCLE column\n",
    "df_crashfile_filtered_2 = df_crashfile.loc[df_crashfile['IN_CIRCLE'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer to (a):\n",
    "df_crashfile_filtered_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge with vehicle file\n",
    "merge_speed_limit = df_crashfile_filtered_2.merge(df_vehiclefile, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer to (b):\n",
    "over_51 = merge_speed_limit.loc[merge_speed_limit['SPEED_LIMIT'] > 51]\n",
    "over_51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer to (c):\n",
    "under_41 = merge_speed_limit.loc[merge_speed_limit['SPEED_LIMIT'] < 41]\n",
    "under_41"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jb7JLx1YKCmP"
   },
   "source": [
    "### Question 7 (15 points)\n",
    "\n",
    "Determine the number of drivers, who got in a car crash, from non-DMV states and list them by total drivers, female and male. Consollidate the final values in one dataframe. <br>\n",
    "`St | Drivers | Female | Male ` <br>\n",
    "`NY | 151034  | 101000 | 50034` <br>\n",
    "`PA | 125131  | 105000 | 20131` <br>\n",
    "...\n",
    "\n",
    "\n",
    "b. Use the states drivers data from the states_drivers.csv to normalize the previous list with number of total drivers for each state. For example, there are 12 million drivers in NY, then the normalized numbers should be:\n",
    "<br>\n",
    "`St |  Drivers  | Normalized` <br>\n",
    "`NY |  151034   | 1.26E-2   ` <br>\n",
    "`PA |  125131   |....       ` <br>\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering by license state\n",
    "df_personfile_nonDMV = df_personfile_merge.loc[(df_personfile_merge['LICENSE_STATE_CODE'] != 'MD') & \n",
    "            (df_personfile_merge['LICENSE_STATE_CODE'] != 'VA') & (df_personfile_merge['LICENSE_STATE_CODE'] != 'DC')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering by person type: D for driver\n",
    "df_personfile_nonDMV_drivers = df_personfile_nonDMV.loc[df_personfile_nonDMV['PERSON_TYPE'] == 'D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer to part (a):\n",
    "#Grouping by state, using agg function to name new columns; inline lambda function to create new gender sum columns\n",
    "df_person_norm = df_personfile_nonDMV_drivers.groupby('LICENSE_STATE_CODE').agg(\n",
    "                                        Drivers=('REPORT_NO','count'), Female= ('SEX_CODE', lambda x: (x == 'F').sum()),\n",
    "                                        Male= ('SEX_CODE', lambda x: (x == 'M').sum()))\n",
    "\n",
    "df_person_norm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_person_norm.index\n",
    "#Observations:\n",
    "#Mislabelled or uncertain state code values: 'AB', 'AK', 'BC', 'GU', 'IT', 'MB', 'MH', 'NB', 'NS', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing states driver data\n",
    "df_norm_values = pd.read_csv('./input_data/states_drivers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing state code json file\n",
    "df_state_codes = pd.read_json('./input_data/states.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining dataframes by index to get driver totals by state code\n",
    "norm_merged = df_norm_values.join(df_state_codes, how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop = ['STATE', 'State', 'Abbrev']\n",
    "norm_merged = norm_merged.drop(columns = col_to_drop)\n",
    "norm_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to sum MALE and FEMALE for normalized values output\n",
    "def sum_genders(male, female):\n",
    "    total = male + female\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying function to create new column of totals\n",
    "norm_merged['TOTAL_DRIVERS'] = norm_merged.apply(lambda row: sum_genders(row['MALE'], row['FEMALE']),axis=1)\n",
    "norm_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing mislabelled state codes from main dataframe\n",
    "state_code_list = list(norm_merged['Code'])\n",
    "other_codes = []\n",
    "\n",
    "for code in list(df_person_norm.index):\n",
    "    if code not in state_code_list:\n",
    "        other_codes.append(code)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "df_person_norm = df_person_norm.drop(index = other_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging\n",
    "df_person_norm  = df_person_norm.merge(norm_merged, left_index = True, right_on = 'Code')\n",
    "df_person_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_person_norm.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to calculate column of normalized values\n",
    "def norm_calc(subset, total):\n",
    "    subset = str(subset).replace(',', '').replace(' ', '')\n",
    "    total = str(total).replace(',', '').replace(' ', '')\n",
    "    norm_val = int(subset)/int(total)\n",
    "    return float(norm_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying in line lambda function to driver totals to calculate new column of normalized values\n",
    "df_person_norm['Normalized'] = df_person_norm.apply(lambda row: norm_calc(row['Drivers'], row['TOTAL_DRIVERS']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer to part (b)\n",
    "final_answer_7 = df_person_norm.set_index('Code')\n",
    "final_answer_7.drop(['index', 'Female', 'Male', 'MALE', 'FEMALE', 'TOTAL_DRIVERS'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZP0MT6tyKCmQ"
   },
   "source": [
    "### Question 8 (15 points)\n",
    "a. On which day of the week (Monday through Sunday) do the most of the crashes happen? <br>\n",
    "b. On which hour of the day (00 - 24) do the most of the crashes happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Will work with the crashfile columns, ACC_DATE, ACC_TIME to answer these questions\n",
    "crash_daytime = pd.DataFrame(df_crashfile[['ACC_DATE', 'ACC_TIME']])\n",
    "crash_daytime['DAY'] = crash_daytime['ACC_DATE'].dt.strftime('%A')\n",
    "crash_daytime.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The time format is not uniform across entries, so I am going to instead split the string value by \":\"\n",
    "crash_daytime['HOUR'] = crash_daytime['ACC_TIME'].str.split(':').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer using describe() function to show frequencies\n",
    "crash_daytime.describe(include = object).T\n",
    "#The most common weekday for accidents is: Friday\n",
    "#The most common hour for accidents is: 17:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4w7e3vLKCmQ"
   },
   "source": [
    "### Question 9 (15 points)\n",
    "a. What is the number of accidents that happened on rainy and clear/cloudy days?\n",
    "b. What is the ration of the number of accidents that happened on rainy days to the number of accidents happened on clear/cloudy days? \n",
    "c. Calculate the number of fatal accidents that occur by each light condition.\n",
    "\n",
    "What does these statistics tell us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary assumptions:\n",
    "##As noted in the initial exploration above, the majority of the weather codes do not match values in the reference documentation\n",
    "##There is a high frequency of code 6.01, which is unusual. \n",
    "##Per the documentation codes: Clear/cloudy = 01, Raining = 03\n",
    "##My assumption is that most of the values for this category were missing or unknown and coded improperly\n",
    "df_crashfile.value_counts(subset = 'WEATHER_CODE', dropna = True, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer to (a)+ (b):\n",
    "#All I can provide is what proportion of accidents occured on rainy days as compared to non-rainy days\n",
    "df_crashfile.value_counts(subset = 'WEATHER_CODE', dropna = True, normalize = True)\n",
    "#12.18% of accidents occured on rainy days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring the light code further\n",
    "df_crashfile.value_counts('LIGHT_CODE', dropna = True, ascending = False)\n",
    "#01 = Daylight\n",
    "#02 = Dawn/Dusk\n",
    "#03-04: Dark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer to (c):\n",
    "#Filtering person data based on injury severity code  = 5 for fatal accidents\n",
    "fatal_accidents = df_personfile_merge.loc[df_personfile_merge['INJ_SEVER_CODE'] > 4]\n",
    "\n",
    "#Dropping columns I don't need on merge\n",
    "fatal_accidents = fatal_accidents.drop(columns = ['PERSON_ID', 'LICENSE_STATE_CODE', 'PERSON_TYPE', 'SEX_CODE', 'DATE_OF_BIRTH']).reset_index()\n",
    "\n",
    "#Merging with crashfile\n",
    "fatal_accidents = fatal_accidents.merge(df_crashfile, how = 'left')\n",
    "\n",
    "#Filtering merged file based on light codes 0-4\n",
    "fatal_accidents = fatal_accidents.loc[fatal_accidents['LIGHT_CODE'] <= 4]\n",
    "\n",
    "#Show count of fatal accidents by light code\n",
    "fatal_accidents.value_counts(subset = 'LIGHT_CODE')\n",
    "\n",
    "#Show proportion of fatal accidents by light code\n",
    "fatal_accidents.value_counts(subset = 'LIGHT_CODE', normalize = True)\n",
    "\n",
    "#The data show that %45.38 of fatal accidents occurred in daylight\n",
    "#The probability of a fatal accident is similar in daylight vs dark conditions based on this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkVa5g4dMRO9"
   },
   "source": [
    "### Question 10 (10 points)\n",
    "Use matplotlib and/or seaborn and show your talent in data visualization. You are free to visualize anything you want regarding this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating data visualizations of accidents and fatalities per month\n",
    "\n",
    "#New columns of accidents month\n",
    "df_crashfile['MONTH'] = df_crashfile['ACC_DATE'].dt.strftime('%B')\n",
    "df_crashfile['MONTH_NUM'] = df_crashfile['ACC_DATE'].dt.strftime('%m')\n",
    "\n",
    "#Creating dataframe of accidents for each month\n",
    "crash_per_month = df_crashfile.groupby('MONTH_NUM').agg({'REPORT_NO':'count'})\n",
    "crash_per_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data for line chart\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'June', 'July', 'Aug', 'Sept', 'Oct', 'Nov', 'Dec']\n",
    "values = list(crash_per_month['REPORT_NO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data for bar chart\n",
    "fatal_accidents_vis = df_personfile_merge.loc[df_personfile_merge['INJ_SEVER_CODE'] > 4]\n",
    "fatal_accidents_vis = fatal_accidents_vis.merge(df_crashfile, how = 'left')\n",
    "fatal_per_month = fatal_accidents_vis.groupby('MONTH_NUM').agg({'REPORT_NO':'count'})\n",
    "fatal_values = list(fatal_per_month['REPORT_NO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Line chart with markers/annotation\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.set_xlabel(\"Month\") \n",
    "ax.set_ylabel(\"Number of Accidents\") \n",
    "ax.plot(months, values, color=\"black\")\n",
    "ax.scatter(months, values, s=50, color=\"blue\", marker = 's')\n",
    "ax.set_ylim(6000, 12000)\n",
    "ax.set_title('Accidents per Month in Maryland, 2017')\n",
    "ax.annotate('Max = 10,579', xy=('Oct',10579), xytext=(50, 0), xycoords='data', textcoords='offset points',\n",
    "            arrowprops=dict(facecolor='red', shrink=0.05))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar plot with annotation\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.set_xlabel(\"Month\") \n",
    "ax.set_ylabel(\"Number of Fatalities\") \n",
    "ax.bar(months, fatal_values, color = 'm')\n",
    "ax.set_ylim(0, 80)\n",
    "ax.set_title('Vehicle Fatalities per Month in Maryland, 2017')\n",
    "ax.annotate('Max = 57', xy=('July', 57), xytext=(50, 0), xycoords='data', textcoords='offset points',\n",
    "            arrowprops=dict(facecolor='red', shrink=0.05))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTEBOOK END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_H1rO_6KCmR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "DATA601_Project1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
